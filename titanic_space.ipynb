{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebd5b8b2-10f5-47c7-861e-acca3a614c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaceship-titanic.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c spaceship-titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2535e8ff-97e3-4ee3-b1d6-b045963c09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf4d8db-f818-45cd-8e67-d12186e7a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I am going for a very simple model with dropping all the nan values and converting the categorical objects into dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6d44cb6-1469-4c67-b4a8-a7db42b2c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class tit_space(Dataset):\n",
    "    def __init__(self, to_ = 'train'):\n",
    "        df = pd.read_csv(f'spaceship-titanic/{to_}.csv')\n",
    "\n",
    "        df.dropna(inplace = True)\n",
    "        df.drop(['Name', 'PassengerId', 'Cabin'], axis = 1, inplace = True)\n",
    "        df = pd.get_dummies(df, columns = ['HomePlanet', 'Destination'], drop_first = True)\n",
    "        df.replace([False, True], [0,1], inplace = True)\n",
    "        df = (df-df.min())/(df.max() - df.min())\n",
    "        # print(df)\n",
    "        self.label = torch.tensor(df.pop('Transported').to_numpy(), dtype = torch.float)\n",
    "        self.df = torch.tensor(df.to_numpy(), dtype = torch.float)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.df[idx], self.label[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32af5fd0-d925-4772-9421-e1a51cc53f22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000, 0.4937, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
       "         0.0000, 0.0000, 1.0000]),\n",
       " tensor(0.))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "tit_data = tit_space()\n",
    "train_dataset, test_dataset = random_split(tit_data, [0.9, 0.1])\n",
    "len(train_dataset), len(test_dataset)\n",
    "tit_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c5b8aeb-5d8a-4ee9-b18e-6748eee9059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter----\n",
    "\n",
    "batch_size = 32\n",
    "lr = 1e-4\n",
    "epoch = 11\n",
    "batch_eval_inter = 100\n",
    "eval_train_test = 10\n",
    "# eval_test = 10\n",
    "\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e81a3a-9e98-488e-85ce-3e1428eb0102",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9865c9-dd4b-4ec6-987a-940caac99adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c3473c0-a7ed-40de-97bc-25be4e1276e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "torch.manual_seed(42)\n",
    "class space_finder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(space_finder, self).__init__() \n",
    "        self.l1 = nn.Sequential(\n",
    "                nn.Linear(12, 100),\n",
    "                nn.BatchNorm1d(100),\n",
    "                nn.ReLU())\n",
    "        \n",
    "        self.l2 = nn.Sequential(\n",
    "                nn.Linear(100, 200),\n",
    "                nn.BatchNorm1d(200),\n",
    "                nn.ReLU())\n",
    "        self.l3 = nn.Sequential(\n",
    "                nn.Linear(200, 500),\n",
    "                nn.BatchNorm1d(500),\n",
    "                nn.ReLU())\n",
    "        self.l4 = nn.Sequential(\n",
    "                nn.Linear(500, 200),\n",
    "                nn.BatchNorm1d(200),\n",
    "                nn.ReLU())\n",
    "        self.l5 = nn.Sequential(\n",
    "                nn.Linear(200, 100),\n",
    "                nn.BatchNorm1d(100),\n",
    "                nn.ReLU())\n",
    "        self.logits = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = self.l1(x)\n",
    "        x2 = self.l2(x1)\n",
    "        x = self.l3(x2)\n",
    "        x = x2 + self.l4(x)\n",
    "        x = x1 + self.l5(x)\n",
    "        logits = self.logits(x)\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0d08765-2150-48a0-a1dc-1bd0ae14167d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1d220893-1076-4e76-9296-2c642060938c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "space_finder.__init__() missing 3 required positional arguments: 'n_embd', 'n_heads', and 'n_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[139], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mspace_finder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mTypeError\u001b[0m: space_finder.__init__() missing 3 required positional arguments: 'n_embd', 'n_heads', and 'n_layers'"
     ]
    }
   ],
   "source": [
    "model = space_finder()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "cfa3748b-af51-4f89-8b35-666434e0565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experimentation\n",
    "# torch.manual_seed(42)\n",
    "# x = torch.rand((1,5,12), device = device)\n",
    "# y = model(x)\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "0c63bfa2-9ba6-4b13-8a20-af59712610a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3fcdd56-1d45-4145-b5cd-b875722a5c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_loop(i, see_batch_loss = False):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    y_true = []\n",
    "    y_preds = []\n",
    "    for batch, (data, label) in enumerate(train_loader):\n",
    "        data , label = data.to(device), label.to(device)\n",
    "        # print(data.shape)\n",
    "        logits = model(data)\n",
    "        # preds = preds.argmax(dim = -1)\n",
    "        # print(logits)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(logits.view(-1), label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sche\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.sigmoid(logits).view(-1) > 0.5\n",
    "        y_true.extend(label.cpu().tolist())\n",
    "        y_preds.extend(preds.detach().cpu().tolist())\n",
    "        \n",
    "        if see_batch_loss:\n",
    "            if batch%eval_train == 0:\n",
    "                print(f'Batch_Loss_{batch} : {loss.item()}')\n",
    "\n",
    "    if i%eval_train_test==0:\n",
    "        val_loss, val_acc = test_loop(test_loader)\n",
    "        print(f'Epoch {i+1}: train_loss: {(total_loss/len(train_loader)):.4f}, train_acc: {(accuracy_score(y_true, y_preds)):.4f}, val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}')\n",
    "        \n",
    "\n",
    "\n",
    "def test_loop(dataset):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        y_true = []\n",
    "        y_preds = []\n",
    "        for data, label in dataset:\n",
    "            data , label = data.to(device), label.to(device)\n",
    "            logits = model(data)\n",
    "    \n",
    "            loss = loss_fn(logits.view(-1), label)\n",
    "            \n",
    "            total_loss+=loss.item()\n",
    "            preds = torch.sigmoid(logits).view(-1) > 0.5\n",
    "            y_true.extend(label.cpu().tolist())\n",
    "            y_preds.extend(preds.detach().cpu().tolist())\n",
    "                              \n",
    "    return total_loss/len(test_loader), accuracy_score(y_true, y_preds)\n",
    "    # print(f'val_loss: {total_loss/len(test_loader)}, val_acc: {accuracy_score(y_true, y_preds)}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "37869b60-094e-4bf7-a0f0-18d44b2afa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.3913, train_acc: 0.8105, val_loss: 0.4382, val_acc: 0.8061\n",
      "Total_time: 24.17899179458618\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for i in range(epoch): \n",
    "    train_loop(i)\n",
    "    # break\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(f'Total_time: {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f6ffa-9b59-40bc-9a9b-b63c50c744d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I trained the model on the 5 layer mlp and the result was not that great i got stuck at 80% and I even trained it for 1000 epochs!\n",
    "# Now I will try to use skip connections, lets see..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4819c3b-2f10-4f0f-af67-181b03373500",
   "metadata": {},
   "source": [
    "#### Lets try a transformer model here!!!!\n",
    "1. First layer will be layer norm\n",
    "2. Second will be attention layer\n",
    "   1. First I will try for a single head attention\n",
    "   2. Multihead attention\n",
    "   3. multiblocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4b293476-f97a-48e5-9cbf-33ea27a57e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter----\n",
    "\n",
    "batch_size = 256\n",
    "lr = 1e-2\n",
    "epoch = 100\n",
    "batch_eval_inter = 100\n",
    "eval_train_test = 10\n",
    "n_embd = 512\n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "# eval_test = 10\n",
    "\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1efafccc-31ac-43c5-8528-37a3d1351ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "06f97485-cc49-4676-bcc9-0af0fc074da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2bd31fc7-db80-4212-9e20-d9ef2a9db921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_loop(i, see_batch_loss = False):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    y_true = []\n",
    "    y_preds = []\n",
    "    for batch, (data, label) in enumerate(train_loader):\n",
    "        data , label = data.to(device), label.to(device)\n",
    "        # print(data.shape)\n",
    "        logits = model(data)\n",
    "        # preds = preds.argmax(dim = -1)\n",
    "        # print(logits)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(logits.view(-1), label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.sigmoid(logits).view(-1) > 0.5\n",
    "        y_true.extend(label.cpu().tolist())\n",
    "        y_preds.extend(preds.detach().cpu().tolist())\n",
    "        \n",
    "        if see_batch_loss:\n",
    "            if batch%eval_train == 0:\n",
    "                print(f'Batch_Loss_{batch} : {loss.item()}')\n",
    "\n",
    "    if i%eval_train_test==0:\n",
    "        val_loss, val_acc = test_loop(test_loader)\n",
    "        print(f'Epoch {i+1}: train_loss: {(total_loss/len(train_loader)):.4f}, train_acc: {(accuracy_score(y_true, y_preds)):.4f}, val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}')\n",
    "        \n",
    "\n",
    "\n",
    "def test_loop(dataset):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        y_true = []\n",
    "        y_preds = []\n",
    "        for data, label in dataset:\n",
    "            data , label = data.to(device), label.to(device)\n",
    "            logits = model(data)\n",
    "    \n",
    "            loss = loss_fn(logits.view(-1), label)\n",
    "            \n",
    "            total_loss+=loss.item()\n",
    "            preds = torch.sigmoid(logits).view(-1) > 0.5\n",
    "            y_true.extend(label.cpu().tolist())\n",
    "            y_preds.extend(preds.detach().cpu().tolist())\n",
    "                              \n",
    "    return total_loss/len(test_loader), accuracy_score(y_true, y_preds)\n",
    "    # print(f'val_loss: {total_loss/len(test_loader)}, val_acc: {accuracy_score(y_true, y_preds)}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e22d1dde-e616-4aeb-9fcd-4c321b64bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Head\n",
    "from torch.nn import functional as F\n",
    "class sa_layer(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        \n",
    "        super(sa_layer, self).__init__()\n",
    "        self.q = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.k = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.v = nn.Linear(n_embd, head_size, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "\n",
    "        wei = q @ k.transpose(0,1) * 100  ** -0.5\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "\n",
    "        v = self.v(x)\n",
    "\n",
    "        out = wei @ v\n",
    "\n",
    "        return out\n",
    "\n",
    "class multihead_attention(nn.Module):\n",
    "    def __init__(self, n_embd, head_size):\n",
    "        \n",
    "        super(multihead_attention, self).__init__()\n",
    "        self.heads = nn.ModuleList([sa_layer(head_size) for _ in range(n_heads)])\n",
    "        self.ll = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], axis = -1)\n",
    "        out = self.ll(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class blocks(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads):\n",
    "        super(blocks, self).__init__()\n",
    "    \n",
    "        head_size = n_embd // n_heads\n",
    "        self.ln1 = nn.LayerNorm(n_embd)        \n",
    "        self.multihead = nn.Sequential(*[multihead_attention(n_embd, head_size) for _ in range(n_layers)])\n",
    "        \n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ffw = nn.Sequential(\n",
    "                    nn.Linear(n_embd, n_embd * 4),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(n_embd * 4, n_embd),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        multihead = x + self.multihead(self.ln1(x))\n",
    "        return multihead + self.ffw(self.ln2(multihead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3e2a6179-096e-490e-ac39-c9619274387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "torch.manual_seed(42)\n",
    "class space_finder(nn.Module):\n",
    "    def __init__(self, n_embd, n_heads, n_layers):\n",
    "        super(space_finder, self).__init__()\n",
    "        self.l1 = nn.Linear(12, n_embd)\n",
    "               \n",
    "        self.blocks = blocks(n_embd , n_heads)\n",
    "\n",
    "        self.logits = nn.Linear(n_embd, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.l1(x)\n",
    "\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        logits = self.logits(x)\n",
    "        \n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6e73e47e-f9d7-4cc1-8623-16dae49468f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "space_finder(\n",
       "  (l1): Linear(in_features=12, out_features=512, bias=True)\n",
       "  (blocks): blocks(\n",
       "    (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (multihead): Sequential(\n",
       "      (0): multihead_attention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x sa_layer(\n",
       "            (q): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=64, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (ll): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (1): multihead_attention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x sa_layer(\n",
       "            (q): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=64, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (ll): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (2): multihead_attention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x sa_layer(\n",
       "            (q): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=64, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (ll): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (3): multihead_attention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x sa_layer(\n",
       "            (q): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=64, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (ll): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (4): multihead_attention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x sa_layer(\n",
       "            (q): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=64, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (ll): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (5): multihead_attention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x sa_layer(\n",
       "            (q): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=64, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=64, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (ll): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (ffw): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (logits): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = space_finder(n_embd, n_heads, n_layers)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2e9a5e64-21eb-4d3c-a032-8ef1023af7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 0.7199, train_acc: 0.3690, val_loss: 0.7200, val_acc: 0.3727\n",
      "Epoch 11: train_loss: 0.7203, train_acc: 0.3687, val_loss: 0.7200, val_acc: 0.3727\n",
      "Epoch 21: train_loss: 0.7198, train_acc: 0.3685, val_loss: 0.7200, val_acc: 0.3727\n",
      "Epoch 31: train_loss: 0.7205, train_acc: 0.3690, val_loss: 0.7200, val_acc: 0.3727\n",
      "Epoch 41: train_loss: 0.7210, train_acc: 0.3687, val_loss: 0.7200, val_acc: 0.3727\n",
      "Epoch 51: train_loss: 0.7204, train_acc: 0.3687, val_loss: 0.7200, val_acc: 0.3727\n",
      "Epoch 61: train_loss: 0.7208, train_acc: 0.3687, val_loss: 0.7200, val_acc: 0.3727\n",
      "Epoch 71: train_loss: 0.7196, train_acc: 0.3690, val_loss: 0.7200, val_acc: 0.3727\n",
      "Epoch 81: train_loss: 0.7201, train_acc: 0.3688, val_loss: 0.7200, val_acc: 0.3727\n",
      "Epoch 91: train_loss: 0.7199, train_acc: 0.3690, val_loss: 0.7200, val_acc: 0.3727\n",
      "Total_time: 178.02459144592285\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for epoch in range(epoch): \n",
    "    train_loop(epoch)\n",
    "    # break\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(f'Total_time: {end-start}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
